#summary Parser design and evolution

The initial goal was to make cross-platform alternative to unix patch
utility. This utility should not require compiling, must be easy to 
extend and should be run as standalone tool or as server process. That's
why Python was chosen.

Another goal was to make a library for automation of patching tasks as
part of rainforce project to encourage development of tools that will
make "patchwork" intuitive and easy.

And the last goal was to reinvert line parser theory from scratch, because
you need to reinvent things to prove that there are no better ways to do
something (or to understand what do you need to look for).


== Design decisions ==

==== Patch target file if source is not found ====

If the source file is not found while patching, *patch.py* tries to patch
target filename. This logic is used to process old style manual patches
that were made by comparing backup file (with some fancy .old extension)
with modified version that holds original name.

==== "Already patched" detection ====

_unified diff format_ doesn't allow to correctly validate if file is
already patched, but only to check if it could be patched. It is because
in some rare cases it is possible to apply the same patch several times
(see 04check_patched example in tests).

Only checksum can reliably detect if file is not patched, but in this case
linefeed differences or minor changes in file will mark file as
non-patchable, while in fact it could be possible if these minor changes
do not intersect with unified diff contents.  This is the feature of
unified format is that non-conlicting patches can be applied in different
order. Some conflicts can even be resolved automatically (not by this lib
so far). Version control systems actually do this during merges.


== Parsers overview ==


== Parser no.0 - Brute Force ==

Versions 8.06 up to 10.04.

The process is very straightforward. Main cycle reads one line at a time
from the input stream. Detects to which part of Diff it belongs (free
header, filename header, hunk) and parses it into corresponding data
structure. The line is either processed of discarded at the end of each
cycle. This guarantees no endless loops or resursions as long as input
stream is finite.

The parser code is one big `for` loop with a series of "parsing blocks"
at the root level. Each parsing block use `if` check if it should process
the current line. Parsing block then does the actual job of extracting text
into Python structures. Local boolean
variables are used for tracking current parser state. Initially named
`header`, `filenames`, `hunkhead`, `hunkbody` after the regions in unified
diff format they showed where parser is located at any current moment. For
recovery from corrupted or invalid data a new `hunkskip` was added.

However, sometimes parser doesn't know where it is located. For example,
when hunk ends, there can be start of another hunk, another file, end of
stream or just garbage. This is where things become complicated.

At the start of new `for` cycle a new line is read and it should be
processed until the end of cycle. `if`s in main cycle check state variables
to dispatch the line into their parsing block. They do not analyze the line
itself (i.e. "context").  When block finishes processing, it switches state
to the next one. So, if there are could be one of several states, this
parsing block should decide which one to choose based on line context.

Checks in the main cycle are simple, but checks in parsing blocks can
become sophisticated if there are more than one state to choose. Not only
such block should know that the line doesn't belong to it, but it also
should be able to analyze it to decide to which successor block it should
be passed.

More than that - the line is disposed at the end of the cycle, so if the
block decides to switch state, because line doesn't belong to it, this
block should be located before the owner of this line. Only then the owner
has a chance to get it when control is returned in the main cycle. That's
why `hunkskip` block used for error recovery is located after `hunkbody`
parser and before `filenames` and `hunkhead` parsing blocks.

Such parser with lines that "fall through" arranged parsing blocks may be
the fastest possible implementation. There are no calls, no repeated checks
after state switching. But the code is hard to read, it is not easily
extensible due to these implicit connections. But unified diff format is
simple and such optimization stil lcould be the way to go in the future.


To summarize:
- state checks are run for every line of the input stream;
- blocks are not explicitly chained 
- every parsing block knows to which state it should switch after
  processing;
- state switching checks can become complicated if there is choice;
- line should be processed until the end of the cycle;
- blocks should be arranged in specific order to intercept the line before
  the end of cycle;
- if it is impossible to rearrange blocks in the appropriate order then a
  "decider" block that doesn't do any parsing is needed (e.g `hunkskip`).

Absence of function calls should speed up things a little. They probably
could make block chaining explicit, but this exposes parser to stack
depletion problem. It is not actual for this specific parser, where amount
of parsed data in memory is bigger anyway, but it is still worthy to keep
this non-recursive and stackless.

Development of parser is complicated, because it is hard to trace state
transformations and estimate how new modifications affect parsing.


== Parser no.1 - Block Context ==

Versions ___

So the first thing to make parser code more clean and maintainable is to
allow parser blocks fetch lines from input stream directly without waiting
the main cycle to complete. Block reads as many lines as it wants, and
switches state when finished. All lines that belong to this block could be
called "block context" and are not exposed to main `if` cycle. So there are
less chances that line that should be processed in this part of the parser
is intercepted by some other rule.

Let's call this feature of Parser no.1 "isolation of block context".

However, block context can not be fully separated. Sometimes decision about
switching state depends on the read line itself, and this line may already
belong other "block context". Current block should switch state to give
this line to its owner. For example, when header parser reads line that
starts with "--- " - it should switch state and pass this line to filename
parser. The filename parser is the owner block that should be called before
the end of the cycle to avoid line being overwritten. In case of `if`s
structure that means owner's block check should be located under part of
the code that switched state.

So each block still knows about the next state or states. It should make a
decision where to switch next. Hence it should know about "block contexts"
of its successors. This bloats and complicates code.

To simplify the code, it is possible to prevent blocks from analysing
each other's context for making switch. The block should only check that
line doesn't belong to its context and switch state to pass control
further.

"block context separation" is another feature of Parser no.2.

It not a complete separation in the sence that header parser still knows
that line starting with "--- " doesn't belong to it. It switches state by
setting `headscan` to False and `filescan` to True and that's all. No
parsing block makes assumptions or decisions where to pass the processing.
There is only one way to switch from parsing block. If there should be a
decision what is to be parsed next, then this decision should be made by
a piece of code that doesn't parse, but just analyzes context to make a
switch. The problem with arranging pieces in correct order still persists.

To solve rearrangement problem it is necessary to either reinvent GOTO or
to be able to reinsert analyzed string back into stream for fetching in
the next cycle after state has already been switched.

It is possible to illustrate "chicken and egg" problem when two blocks
analyze each others context to pass control from one to other and could
not be rearranged, because at the end of processing the current block
should always be higher than the other, because it needs to pass the line.

The solution can be in:
- skip fetching line on the next cycle
- reinsert line into the stream
- wrapper switch that judges who gets the next line, this requires one more
  state variable, and that blocks process lines one by one
  
Reinserting lines can provide some performance overhead, wrapper switch
complicates parser, so skip line fetching may be a good solution.


So, this parser doesn't use "pass-down-this-line" schemes and hence is not
vulnerable to "chicken and egg" when line should processed in the same
cycle to avoid being overwritten. Every block requests as many lines as it
needs, switches state to "finished" and returns control to the beginning of
the main cycle. Main cycle analyzes state and passes control to the next
appropriate block. 

"pass-down-this-line" problem can be alleviated by a global flag `skipline`
that instructs parser not to.
